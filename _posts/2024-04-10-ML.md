---
title: ML笔记（更新中）
date: 2024-04-10 11:20:05
tags:
- Machine Learning
categories: 
- Note
---

wx+b 成本函数 代价函数

线性回归指的是单参数变量

w、b是要定义的系数或者权重

1/2m 2是为了简约，求导消2

平方误差成本函数

梯度下降实际就是不断逼近最小成本，步长就是阿尔法学习率

学习率太小收敛慢，太大发散

梯度下降同步更新要搞出两个temp变量duo'yua

多元线性回归

np.dot 是点积 硬件层面上加速比for快

不迭代可以用法方程（但只能用在线性回归）

线性回归不适合分类是因为：假设不断增加样本量而分界值是0.5会导致有些消极样本变成积极
（直线右移）

逻辑回归实际上是用来分类的

sigmoid 函数是用1/[1+(e^-z)] 实现的



