---
title: 数分面试准备资料（过时）
date: 2022-03-21 11:20:05
tags:
- Data Analysis
categories: 
- Note
---

# 1. 数分面经


## 1.1. 正则化

定义：对某一问题加以先验限制或约束以达到特定目的的手段或操作。

原理：对损失函数加上约束，从而减少模型的方差提高泛化能力。

损失函数是估量模型预测值与真实值的不一致程度。

## 1.2. L1正则化
最小绝对值误差

会产生稀疏矩阵用于特征选择，可以关注少数特征对这个模型有贡献的

## 1.3. L2正则化
最小平方误差

倾向于产生更小更分散的权重向量

让模型做决策时考虑更多特征

## 1.4. 相同点
本质都是防止过拟合

# 2. 过拟合
定义
模型在训练集上表现优越，但在验证集或测试集上表现不佳。

## 2.1. 过拟合解决方法
增加样本数量
深度学习
正则化
简化模型（PCA降维或提取特征）
训练过程提前终止
K-folds交叉验证

# 3. 有/无 监督学习 和 区别
## 3.1. 有监督学习
分类都是有监督学习

决策树
随机森林
极端梯度递增（XGB）
K近邻（KNN）
支持向量机（SVM）
朴素贝叶斯

## 3.2. 无监督学习
聚类都是无监督，靠的是启发式搜索

k-means
PCA主成分分析
apriori
## 3.3. 区别
- 监督学习给数据加了标签，无监督学习没有加标签
- 有监督学习方法必须要有训练集和测试样本，在训练集中找规律从而对测试样本使用
- 无监督学习直接在数据中寻找规律

# 4. 决策树
定义：
决策树是一种对样本进行分类的树形结构，也能够进行回归预测。

决策树主要包含三种结点（根结点：初始节点；叶结点：最终分类结果结点；内节点：树内部进行判断的条件节点-即特征）

在决策树中，每个样本都只能被一条路径覆盖。

## 4.1. 基本思想
信息熵
可以衡量特征到底多不确定。信息熵越小不确定性越低。

以信息熵为度量构造一棵熵值下降最快的树。到叶子节点处的熵值为零，此时每个叶结点中的实力都属于同一类。


## 4.2. 优缺点
### 4.2.1. 优点
- 易于理解和解释和可以可视化分析，容易提取出规则
- 速度快、计算量相对较小
- 可以处理连续和种类字段
### 4.2.2. 缺点
- 数据及特征很多时，容易过拟合
- 忽略了特征之间的相关性

# 5. 决策树三种分类算法

决策树学习算法包括三部分：特征选择、树的生成和树的剪枝。

常用的算法有ID3、C4.5和CART决策树，分别基于信息增益、信息增益率和基尼指数划分。

信息增益：使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。信息增益=使用该特征分类前的信息熵-使用后的信息熵

信息增益率：信息增益率=信息增益/特征熵

（基尼）Gini指数：总体内包含的类别越杂乱，GINI指数就越大（跟熵的概念很相似只不过形容相反）。

## 5.1. ID3算法
使用信息增益进行特征选择。当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵值是一定的，因此信息增益比较偏向取值较多的特征。

## 5.2. C4.5算法
使用信息增益率进行特征选择，克服了信息增益选择特征的时候偏向于特征个数较多的不足。

并且可以处理连续性数据和缺失值。

## 5.3. CART算法
分类回归树，既可以用于分类也可以用于预测。使用基尼系数进行特征选择。
### 5.3.1. 缺点
不能处理缺失值

## 5.4. 防止决策树过拟合
剪枝

给决策树瘦身，目标不许呀太多判断就可以得到好结果，防止过拟合发
生。

分为预剪枝和后剪枝，由于过拟合大多是数据中的噪声和离群点导致的过分拟合问题。

预剪枝
在构造过程中当某个节点满足简直条件就停止该分支构造。
比如指定树的深度

后剪枝
先构造完整决策树，再通过某些条件遍历树进行剪枝。

# 6. 逻辑回归
## 6.1. 线性回归
利用多个输入变量预测一个输出变量。

逻辑回归则是线性回归通过S函数将结果映射到0，1区间内，做分类判断时，设定一个概率的阈值，大于阈值的样本预测值为1，小于则样本预测为0.

常用场景：
- 广告点击行为预测
- 借款人评估
- 信用评分卡等等

优点：
- 模型输出即为样本的概率分布
- 可以输出表达式且分析各个特征权重
- 在SPSS和Python能够直接调用，且构建的模型可解释性高。

缺点：
- 预测结果呈现S型分布，两端概率变化非常小，中间概率变化剧烈，难找阈值
- 只能处理线性可分类问题
- 拟合能力差

# 7. 随机森林
定义：随机森林的由很多决策树构成 当我们输入一个要预测分类的数据时，每个决策树都会接收数据并产生一个分类结果，然后采用投票机制，认为哪个结果出现的次数最多就归为哪一类。

原理：随机森林是bagging类的集成学习算法，可以利用多个弱学习器投票得到最终答案，其中每个弱学习器的构建方法是从全部样本和全部特征中，有放回的抽取部分样本和特征进行训练。
## 7.1. 构造过程
随机样本的选取→ 随机选取特征→ 构建决策树→ 随机森林投票

## 7.2. 优势
有效解决决策树的过拟合问题，能够处理特征维度很多的数据，并且不需要做特征选择，可以解决回归或者分类问题。

## 7.3. 劣势
模型的可解释性比较差，无法控制模型内部的运行，会在某些噪声较大的分类或回归问题上会过拟合（噪声意思是错误的数据）。

## 7.4. 应用场景 
可以用来预测贷款风险，银行和证券使用较多，调参简单。 

## 7.5. 提问：随机森林随机性体现在哪？
答：随机选取样本、随机选取特征。 

## 7.6. 各类算法比较 
### 7.6.1. Bagging算法
每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。其代表算法是随机森林。

### 7.6.2. Boosting算法
通过迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关。

### 7.6.3. XG Boosting
极端梯度递增是boosting类的集成学习方法，按顺序生成各分类器，后一个分类器的参数会根据前面分类器的残差大小来设置对应的权重，所有弱分类器的结果相加等于预测值。 

## 7.7. bagging和boosting的区别
在样本选择上，bagging方法是有放回的选择样本，所有样本的权重保持不变；boosting方法是每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。

在预测函数上，bagging方法所有弱分类器权重相等；boosting方法中每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

在计算方式上，bagging采用并行计算的方式，同时生成所有弱分类器；boosting采用顺序计算的方式，各分类器只能按顺序生成，后一个分类器的参数会用到前一个分类器的结果。

梯度提升是一种用于回归和分类问题的机器学习技术，它以弱预测模型（通常是决策树）的集合的形式产生预测模型。

## 7.8. 随机森林和gbdt区别

1. 都是由多棵树组成 随机森林是bagging算法 gbdt是boosting算法。
2. bagging算法采取均匀有放回抽样，boosting根据错误率来取样。
3. 对于最终的输出结果而言， 随机森林采用多数投票等； 而 GBDT 则是将所有结果累加起来， 或者加权累加起来。
4. 随机森林的树可以并行生成，gbdt只能串行生成。
5. gbdt对异常值敏感，随机森林对异常值不敏感。

## 7.9. 随机森林和XGBoost的区别
1. 随机森林采用的是bagging算法同时生成多个弱分类器。 xgb使用的是boosting算法，按顺序生成各分类器。
2. XGB会根据特征的重要性进行排序 随机森林是随机选取特征。
3. 随机森林不能处理缺失值 ， XGB可以处理缺失值。
4. 对于最终的输出结果而言，随机森林采用多数投票等；而xgboost则是将所有结果累加起来，或者加权累加起来。
5. 随机森林计算量少 计算速度也更快 并且需要的参数也少，XGB需要的参数多，运行速度较慢。

## 7.10. lgbm和xgb的区别
① xgb采用了预排序方法处理节点分裂，lgbm采用的是直方图优化算法，减少了内存消耗和计算代价。

② 决策树生长策略，XGBoost 采用的是 Level-wise（按层生长） 的树生长策略，LightGBM 采用的是 leaf-wise （按叶子生长）的生长策略， 以最大信息增益为导向。后者进度更高，容易过拟合，所以要控制最大深度。

③ 并行策略对比，XGBoost 的并行主要集中在特征并行上，而 LightGBM 的并行策略分特征并行，数据并行以及投票并行。

# 8. 归一化
## 8.1. 广义常用的归一化方法
最大最小值归一化、去均值/标准差

## 8.2. 作用
无量纲化，提高收敛速度。  无量纲化的意思：消除单位所带来的影响，比如通过这两个特征身高（cm） 体重（克），预测年龄。

## 8.3. 不需要进行归一化的模型 
决策树、GBDT、XGBoost、LightGBM、CatBoost这样的树模型
因为它们在训练的过程中通过信息增益、信息增益率、gini系数来进行树的生长，不涉及梯度下降过程。
梯度下降就是找到让损失函数的误差值最小时候算法取的参数。
树形模型不需要归一化，因为树模型并不关心变量的值，而是关心变量的分布和变量之间的条件概率。

## 8.4. 需要进行归一化
1. 基于距离计算的模型：KNN。
2. 通过梯度下降法求解的模型：线性回归、逻辑回归、支持向量机、神经网络。

## 8.5. 标准化归一化区别
狭义上可以分为标准化和归一化两种。 
### 8.5.1. 标准化

- 标准化是减均值除以标准差，将数据分布变成均值为0标准差为1的分布 不一定是正态分布
- 标准化保留了数据之间的距离，没有保留数据的权重大小
- 适用于分类 聚类算法和大数据

### 8.5.2. 归一化
- 归一化是是减最小值 除以最大值减最小值的差 将数据规约到0,1 或者-1,1区间
- 丢失了数据之间的距离，保留了数据的权重大小
- 适用于小数据以及不涉及距离度量的时候

# 9. AUC
## 9.1. 定义
1. 随机给定一个正样本和负样本，用分类器进行分类和预测，正样本得分大于负样本得分的概率，就是AUC。
2. 横坐标是假阳率 纵坐标是召回率 roc曲线所为成的面积就是AUC的值。 